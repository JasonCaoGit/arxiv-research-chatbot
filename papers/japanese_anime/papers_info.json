{
  "1606.03048v2": {
    "title": "A Minimum Spanning Tree Representation of Anime Similarities",
    "authors": [
      "Canggih Puspo Wibowo"
    ],
    "summary": "In this work, a new way to represent Japanese animation (anime) is presented.\nWe applied a minimum spanning tree to show the relation between anime. The\ndistance between anime is calculated through three similarity measurements,\nnamely crew, score histogram, and topic similarities. Finally, the centralities\nare also computed to reveal the most significance anime. The result shows that\nthe minimum spanning tree can be used to determine the similarity anime.\nFurthermore, by using centralities calculation, we found some anime that are\nsignificant to others.",
    "pdf_url": "http://arxiv.org/pdf/1606.03048v2",
    "published": "2016-06-07"
  },
  "1508.03653v1": {
    "title": "AnimalCatcher: a digital camera to capture various reactions of animals",
    "authors": [
      "Koji Tsukada",
      "Maho Oki",
      "Kazutaka Kurihara",
      "Yuko Furudate"
    ],
    "summary": "People often have difficulty to take pictures of animals, since animals\nusually do not react with cameras nor understand verbal directions. To solve\nthis problem, we developed a new interaction technique, AnimalCatcher, which\ncan attract animals' attention easily. The AnimalCatcher shoots various sounds\nusing directional speaker to capture various reactions of animals. This paper\ndescribes concepts, implementation, and example pictures taken in a zoo.",
    "pdf_url": "http://arxiv.org/pdf/1508.03653v1",
    "published": "2015-04-25"
  },
  "2203.02632v1": {
    "title": "Extracting linguistic speech patterns of Japanese fictional characters using subword units",
    "authors": [
      "Mika Kishino",
      "Kanako Komiya"
    ],
    "summary": "This study extracted and analyzed the linguistic speech patterns that\ncharacterize Japanese anime or game characters. Conventional morphological\nanalyzers, such as MeCab, segment words with high performance, but they are\nunable to segment broken expressions or utterance endings that are not listed\nin the dictionary, which often appears in lines of anime or game characters. To\novercome this challenge, we propose segmenting lines of Japanese anime or game\ncharacters using subword units that were proposed mainly for deep learning, and\nextracting frequently occurring strings to obtain expressions that characterize\ntheir utterances. We analyzed the subword units weighted by TF/IDF according to\ngender, age, and each anime character and show that they are linguistic speech\npatterns that are specific for each feature. Additionally, a classification\nexperiment shows that the model with subword units outperformed that with the\nconventional method.",
    "pdf_url": "http://arxiv.org/pdf/2203.02632v1",
    "published": "2022-03-05"
  },
  "1706.03497v1": {
    "title": "A filter based approach for inbetweening",
    "authors": [
      "Yuichi Yagi"
    ],
    "summary": "We present a filter based approach for inbetweening. We train a convolutional\nneural network to generate intermediate frames. This network aim to generate\nsmooth animation of line drawings. Our method can process scanned images\ndirectly. Our method does not need to compute correspondence of lines and\ntopological changes explicitly. We experiment our method with real animation\nproduction data. The results show that our method can generate intermediate\nframes partially.",
    "pdf_url": "http://arxiv.org/pdf/1706.03497v1",
    "published": "2017-06-12"
  },
  "2312.11568v1": {
    "title": "VectorTalker: SVG Talking Face Generation with Progressive Vectorisation",
    "authors": [
      "Hao Hu",
      "Xuan Wang",
      "Jingxiang Sun",
      "Yanbo Fan",
      "Yu Guo",
      "Caigui Jiang"
    ],
    "summary": "High-fidelity and efficient audio-driven talking head generation has been a\nkey research topic in computer graphics and computer vision. In this work, we\nstudy vector image based audio-driven talking head generation. Compared with\ndirectly animating the raster image that most widely used in existing works,\nvector image enjoys its excellent scalability being used for many applications.\nThere are two main challenges for vector image based talking head generation:\nthe high-quality vector image reconstruction w.r.t. the source portrait image\nand the vivid animation w.r.t. the audio signal. To address these, we propose a\nnovel scalable vector graphic reconstruction and animation method, dubbed\nVectorTalker. Specifically, for the highfidelity reconstruction, VectorTalker\nhierarchically reconstructs the vector image in a coarse-to-fine manner. For\nthe vivid audio-driven facial animation, we propose to use facial landmarks as\nintermediate motion representation and propose an efficient landmark-driven\nvector image deformation module. Our approach can handle various styles of\nportrait images within a unified framework, including Japanese manga, cartoon,\nand photorealistic images. We conduct extensive quantitative and qualitative\nevaluations and the experimental results demonstrate the superiority of\nVectorTalker in both vector graphic reconstruction and audio-driven animation.",
    "pdf_url": "http://arxiv.org/pdf/2312.11568v1",
    "published": "2023-12-18"
  }
}